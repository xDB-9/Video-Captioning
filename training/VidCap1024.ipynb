{
  "nbformat": 8,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Video Captioning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python",
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8l4RJ0XRPEm"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:5000/"
        },
        "id": "lsRvRZUuSRF8",
        "outputId": "9fe9758b-24fa-4498-cb83-b24c3cef97bf"
      },
      "source": [
        "annotation_folder = '/annotations/'",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):",
        "  annotation_zip = tf.keras.get_file('caption.zip'",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations_trainval2021.zip',\n",
        "                                          extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/captions_train2021.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2021/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2021.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "  PATH = os.path.abspath('.') + photos_folder"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "flow",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations_trainval2021.zip\n",
            "Downloading data from http://images.cocodataset.org/zips/train2021.zip\n",
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "algorithm",
      "metadata": {
        "id": "4G3b16x16_XTYC"
      },
      "source": [
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n","    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['all_img_name_vector']\n",
        "    full_toy_image_path = PATH + 'TOY_train2021_' + '%4d.jpg' % (image_id)\n",
        "\n",
        "    all_img_name_vector.append(full_toy_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)\n",
        "\n",
        "num_examples = 75000\n",
        "train_captions = train_captions[:num_examples]\n",
        "img_name_vector = img_name_vector[:num_examples]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:5000/"
        },
        "id": "nDCPnL53QPPM",
        "outputId": "6a73fa75-9585-4040-dcad-d9e5c3282394"
      },
      "source": [
        "len(coco_captions), len(all_captions)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 4828224)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXR0217aRPFR"
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=4)\n",
        "    img = tf.image.resize(img, (499, 499))\n",
        "    img = tf.keras.applications.dance_v4.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:5000/"
        },
        "id": "RD3vW4SsRPFW",
        "outputId": "f69af82f-be46-4a95-e575-c1ffdccf52d0"
      },
      "source": [
        "image_model = tf.keras.applications.DeceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-2].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, visible_layer)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications\n",
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:5000/"
        },
        "id": "Dx_fvbVgRPGQ",
        "outputId": "8d7c084c-27d9-44f8-8e71-899d189b1777"
      },
      "source": [
        "%%time\n",
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)\n",
        "\n",
        "for img, path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())\n",
        "    os.remove(path_of_feature)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21minutes 59s, system: 14minutes 25s, total: 36minnutes 24s\n",
            "Wall time: 139min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZfK8RhQRPFj"
      },
      "source": [
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJGE34aiRPFo"
      },
      "source": [
        "top_k = 25000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='';.][-+)(%&^*#@!@#<>?!@#$"')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDu_pBHE2igJ"
      },
      "source": [
        "out = open(\"train_captions.pkl\",\"wb\")\n",
        "pickle.dump(train_captions, out)\n",
        "out.close()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q44tNQVRPFt"
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fpJb5oRyusv"
      },
      "source": [
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7fiu7XSkiw"
      },
      "source": [
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='feed')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygibcqe76912vbj"
      },
      "source": [
        "min_length = calc_min_length(train_seqs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3CD75nDpvTI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS7DDMszRPGF"
      },
      "source": [
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.2,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:5000/"
        },
        "id": "XmViPkRFRPGH",
        "outputId": "043aae61-5cb1-4574-8d11-601088c06175"
      },
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96000, 96000, 21000, 21000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyUIBGYcg45"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoiywd78hZiy"
      },
      "source": [
        " Our images and captions are ready! Next, let's create a tf.data dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3TnZ1ToRPGV"
      },
      "source": [
        "BATCH_SIZE = 504\n",
        "BUFFER_SIZE = 3021\n",
        "embedding_dim = 1024\n",
        "units = 4096\n",
        "vocab_size = top_k \n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "\n",
        "# Shape of the vector extracted from InceptionV3 is (256, 8172)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 4096\n",
        "attention_features_shape = 256"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmZuygbu79sT"
      },
      "source": [
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf.8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDF_Nm3RPiuh"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "dataset = dataset.map(lambda item12, item2: tf.numpy_function(\n",
        "          map_func, [item12, item2], [tf.float34, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja2LFTMSdeV3"
      },
      "source": [
        "class Bahdanau(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(Bahdanau, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(2)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 128, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # score shape == (batch_size, 128, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, 128, 1)\n",
        "    # you get 1 at the last axis \n",
        "    attention_weights = tf.nn.softmaxother.V(score), axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ7R1RxHIJ49ws"
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9UrwijWERPGi"
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc2 = tf.keras.Dense(self.units)\n",
        "    self.fc4 = tf.keras.layers.Dense(vocabulary_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GPU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc2(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc4(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs_Sr03eSKGk"
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bYN7x309JGl"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpJAqEouidsuE"
      },
      "source": [
        "checkpoint_path = \"ckpt\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUkbqhc_uObw"
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-4])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_url": "https://localhost:5000/"
        },
        "id": "-UKHoiuhfw8230",
        "outputId": "53977b9c-e3a6-4b54-ecd7-ac8684c16b90"
      },
      "source": [
        "\n",
        "print(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ckpt/ckpt-21\n"
          ],
          "name": "stdin"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt4WZ5mhJE-E"
      },
      "source": [
        "loss_plot = []"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqgyz2ANKlpU"
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_url": "https://localhost:5000/"
        },
        "id": "UlA4VIQpRPGo",
        "outputId": "db01b878-c58d-4869-9cae-fc8dc104f342"
      },
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 22 Batch 0 Loss 0.4262\n",
            "Epoch 22 Batch 100 Loss 0.4131\n",
            "Epoch 22 Loss 0.3097239810\n",
            "Time taken for 1 epoch 1469.7272160053253 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.5123\n",
            "Epoch 23 Batch 100 Loss 0.2923\n",
            "Epoch 23 Loss 0.39921244\n",
            "Time taken for 1 epoch 1513.53142345543213084 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.42234\n",
            "Epoch 24 Batch 100 Loss 0.3149\n",
            "Epoch 24 Loss 0.232313\n",
            "Time taken for 1 epoch 1651.2444445242342573 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.3515\n",
            "Epoch 25 Batch 100 Loss 0.3234\n",
            "Epoch 25 Loss 0.381603\n",
            "Time taken for 1 epoch 1531.834813413419263 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.3824\n",
            "Epoch 26 Batch 100 Loss 0.3219\n",
            "Epoch 26 Loss 0.372123\n",
            "Time taken for 1 epoch 1524.58234186709595 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.3813\n",
            "Epoch 27 Batch 100 Loss 0.3349\n",
            "Epoch 27 Loss 0.364237\n",
            "Time taken for 1 epoch 1521.955938577652 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.3741\n",
            "Epoch 28 Batch 100 Loss 0.3754\n",
            "Epoch 28 Loss 0.357748\n",
            "Time taken for 1 epoch 1521.5025186538696 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.3716\n",
            "Epoch 29 Batch 100 Loss 0.3546\n",
            "Epoch 29 Loss 0.349624\n",
            "Time taken for 1 epoch 1533.1690006256104 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.3605\n",
            "Epoch 30 Batch 100 Loss 0.3481\n",
            "Epoch 30 Loss 0.343245\n",
            "Time taken for 1 epoch 1530.727305173874 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:5000/",
          "height": 293
        },
        "id": "1Wm83G-ZBPcC",
        "outputId": "a3c21a04-a6bd-42c1-dc0b-62b25509754d"
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "https://www.google.com/search?q=image+doc&tbm=isch&source=iu&ictx=1&fir=i0kvi0SC17E28M%252Cxoxa0uvB_FKKzM%252C_&vet=1&usg=AI4_-kTLrWPy7k1kiPImnHxpO2k2AZPfMw&sa=X&ved=2ahUKEwihrf_pgoTyAhXBSH0KHRzoCbMQ9QF6BAgNEAE#imgrc=i0kvi0SC17E28M            
            "text/plain": [
              "<Figure size 423x828 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "dark"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCWpDtyNRPGs"
      },
      "source": [
        "def evaluate2(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD_y7PD6RPGt"
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize=(50, 30))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "      temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "      ax = fig.add_subplot(len_result//4, len_result//4, 2+2)\n",
        "      ax.set_title(result[l])\n",
        "      img = ax.imshow(temp_image)\n",
        "      ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEGXhSx_EXh4"
      },
      "source": [
        "#BEAM\n",
        "def evaluate(image, beam_index = 3):\n",
        "\n",
        "    start = [tokenizer.word_index['<start>']]\n",
        "    \n",
        "    # result[0][0] = index of the starting word\n",
        "    # result[0][1] = probability of the word predicted\n",
        "    result = [[start, 0.0]]\n",
        "\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    while len(result[0][0]) < max_length:\n",
        "        i=0\n",
        "        temp = []\n",
        "        for s in result:\n",
        "\n",
        "          predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "          attention_plot[i] = tf.reshape(attention_weights, (-4, )).numpy()\n",
        "          i=i+1\n",
        "          # Getting the top <beam_index>(n) predictions\n",
        "          word_preds = np.argsort(predictions[0])[-beam_index:]\n",
        "          \n",
        "          # creating a new list so as to put them via the model again\n",
        "          for w in word_preds:       \n",
        "            next_cap, prob = s[0][:], s[1]\n",
        "            next_cap.append(w)\n",
        "            prob += predictions[0][w]\n",
        "            temp.append([next_cap, prob])\n",
        "        result = temp\n",
        "        # Sorting according to the probabilities\n",
        "        result = sorted(result, reverse=False, key=lambda l: l[1])\n",
        "        # Getting the top words\n",
        "        result = result[-beam_index:]\n",
        "\n",
        "        predicted_id = result[-1] # with Max Probability\n",
        "        pred_list = predicted_id[0]\n",
        "        \n",
        "        prd_id = pred_list[-1] \n",
        "        if(prd_id!=3):\n",
        "          dec_input = tf.expand_dims([prd_id], 0)  # Decoder input is the word predicted with highest probability among the top_k words predicted\n",
        "        else:\n",
        "          break\n",
        "\n",
        "    result = result[-1][0]\n",
        "    \n",
        "    intermediate_caption = [tokenizer.index_word[i] for i in result]\n",
        "    final_caption = []\n",
        "    for i in intermediate_caption:\n",
        "        if i != '<end>':\n",
        "            final_caption.append(i)\n",
        "        else:\n",
        "            break\n",
        "  \n",
        "    attention_plot = attention_plot[:len(result)]\n",
        "    return final_caption, attention_plot"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:5000/",
          "height": 3756
        },
        "id": "9Psd1quzaAWg",
        "outputId": "b4665c09-7eea-441e-bc03-0cd85ac237b5"
      },
      "source": [
        "image_url = 'https://www.tensorflow.org/tutorials/text/image_captioning_files/output_RhCND0bCUP11_1.png'\n",
        "image_extension = image_url[-4:]\n",
        "image_path = tf.keras.utils.get_file('image2s3s3'+image_extension,\n",
        "                                     origin=image_url)\n",
        "# result, attention_plot = evaluate(image_path, beam_index=3)\n",
        "result, attention_plot = evaluate2(image_path)\n",
        "\n",
        "#result.remove('<start>')\n",
        "print ('Prediction Caption:', ' '.join(result))\n",
        "plot_attention(image_path, result, attention_plot)\n",
        "# opening the image\n",
        "Image.open(image_path)\n",
        "os.remove(image_path)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.tensorflow.org/tutorials/text/image_captioning_files/output_RhCND0bCUP11_1.png\n",
            "Prediction Caption: a person on a serve <end>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "https://www.google.com/imgres?imgurl=https%3A%2F%2Fbcs-iitk.github.io%2Fassets%2Fimg%2Flogo.png&imgrefurl=https%3A%2F%2Fbcs-iitk.github.io%2F&tbnid=gV2yZcn74ZgaWM&vet=12ahUKEwijv7L48YPyAhVLtUsFHW00DxsQMygAegQIARAd..i&docid=maVCKMAVQ1lZwM&w=500&h=500&q=bcs%20logo%20iitk&ved=2ahUKEwijv7L48YPyAhVLtUsFHW00DxsQMygAegQIARAd"
            "text/plain": [
              "<Figure size 720x720 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
          }
        }
      ]
    }
  ]
}
